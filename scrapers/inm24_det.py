#!/usr/bin/env python3
"""
Inmuebles24 Unico Professional Scraper - PropertyScraper Dell710
Scraper profesional para detalles de propiedades individuales de Inmuebles24
Optimizado para Dell T710 con capacidades de resilencia
"""

import os
import sys
import json
import time
import csv
import logging
import pickle
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import argparse

# Selenium imports
from seleniumbase import SB
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

class Inmuebles24UnicoProfessionalScraper:
    """
    Scraper profesional para detalles de propiedades individuales de inmuebles24.com
    Segunda fase del proceso de scraping - procesa URLs individuales
    """
    
    def __init__(self, urls_file=None, headless=True, max_properties=None, resume_from=None, operation_type='venta'):
        self.urls_file = urls_file
        self.headless = headless
        self.max_properties = max_properties
        self.resume_from = resume_from or 0
        self.operation_type = operation_type
        
        # ConfiguraciÃ³n de paths
        self.setup_paths()
        
        # ConfiguraciÃ³n de logging
        self.setup_logging()
        
        # Checkpoint system
        self.checkpoint_file = self.checkpoint_dir / f"inmuebles24_unico_{operation_type}_checkpoint.pkl"
        self.checkpoint_interval = 25  # Guardar cada 25 propiedades procesadas
        
        # Cargar URLs
        self.property_urls = self.load_urls()
        self.properties_data = []
        
        # Performance metrics
        self.start_time = None
        self.properties_processed = 0
        self.successful_extractions = 0
        self.errors_count = 0
        
        self.logger.info(f"ðŸš€ Iniciando Inmuebles24 Unico Professional Scraper")
        self.logger.info(f"   URLs file: {urls_file}")
        self.logger.info(f"   Total URLs: {len(self.property_urls)}")
        self.logger.info(f"   Operation: {operation_type}")
        self.logger.info(f"   Max properties: {max_properties}")
        self.logger.info(f"   Resume from: {resume_from}")
        self.logger.info(f"   Headless: {headless}")
    
    def setup_paths(self):
        """Configurar estructura de paths del proyecto"""
        self.project_root = Path(__file__).parent.parent
        
        # Mapear operation_type a nueva nomenclatura
        operation_mapping = {
            'renta': 'ren',
            'venta': 'ven', 
            'venta-d': 'ven-d',
            'venta-r': 'ven-r'
        }
        self.operation_folder = operation_mapping.get(self.operation_type, 'ven')
        
        self.data_dir = self.project_root / 'data' / 'inm24' / self.operation_folder
        self.logs_dir = self.project_root / 'logs'
        self.checkpoint_dir = self.project_root / 'logs' / 'checkpoints'
        
        # Crear directorios si no existen
        for directory in [self.data_dir, self.logs_dir, self.checkpoint_dir]:
            directory.mkdir(parents=True, exist_ok=True)
    
    def setup_logging(self):
        """Configurar sistema de logging profesional"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = self.logs_dir / f"inmuebles24_unico_{self.operation_type}_professional_{timestamp}.log"
        
        # ConfiguraciÃ³n de logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s | %(levelname)8s | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.log_file = log_file
    
    def load_urls(self) -> List[str]:
        """Cargar URLs desde archivo o desde el CSV mÃ¡s reciente de inmuebles24_professional"""
        urls = []
        
        if not self.urls_file:
            # Buscar el CSV mÃ¡s reciente del primer script (I24_URLs_*.csv)
            pattern = f"I24_URLs_*.csv"
            csv_files = list(self.data_dir.glob(f"**/{pattern}"))
            if csv_files:
                latest_csv = max(csv_files, key=lambda x: x.stat().st_mtime)
                self.logger.info(f"ðŸ“‚ Usando CSV mÃ¡s reciente: {latest_csv}")
                
                # Extraer URLs del CSV
                try:
                    import pandas as pd
                    df = pd.read_csv(latest_csv)
                    if 'link' in df.columns:
                        urls = df['link'].dropna().tolist()
                        self.logger.info(f"ðŸ“‚ ExtraÃ­das {len(urls)} URLs del CSV")
                    else:
                        self.logger.error("âŒ Columna 'link' no encontrada en el CSV")
                except Exception as e:
                    self.logger.error(f"âŒ Error leyendo CSV: {e}")
            else:
                # Buscar archivo de URLs de texto con nueva nomenclatura
                pattern = f"I24_URLs_*.txt"
                url_files = list(self.data_dir.glob(f"**/{pattern}"))
                if url_files:
                    self.urls_file = max(url_files, key=lambda x: x.stat().st_mtime)
                    self.logger.info(f"ðŸ“‚ Usando archivo de URLs: {self.urls_file}")
                else:
                    self.logger.error("âŒ No se encontrÃ³ archivo de URLs ni CSV")
                    return []
        
        # Si tenemos archivo de URLs especÃ­fico, usarlo
        if self.urls_file and not urls:
            try:
                self.logger.info(f"ðŸ” DEBUGGING: Analizando archivo {self.urls_file}")
                
                # Verificar si es un archivo CSV
                if str(self.urls_file).endswith('.csv'):
                    self.logger.info(f"ðŸ“„ DEBUGGING: Detectado archivo CSV")
                    import csv
                    with open(self.urls_file, 'r', encoding='utf-8') as f:
                        # Leer todas las lÃ­neas para debugging
                        f.seek(0)
                        all_lines = f.readlines()
                        self.logger.info(f"ðŸ“Š DEBUGGING: Total lÃ­neas en archivo: {len(all_lines)}")
                        self.logger.info(f"ðŸ“‹ DEBUGGING: Primera lÃ­nea (header): {all_lines[0].strip()}")
                        if len(all_lines) > 1:
                            self.logger.info(f"ðŸ“‹ DEBUGGING: Segunda lÃ­nea (ejemplo): {all_lines[1].strip()}")
                        
                        # Reiniciar archivo y leer con CSV reader
                        f.seek(0)
                        reader = csv.DictReader(f)
                        headers = reader.fieldnames
                        self.logger.info(f"ðŸ“‹ DEBUGGING: Headers detectados: {headers}")
                        
                        row_count = 0
                        url_count = 0
                        
                        for row_index, row in enumerate(reader, 1):
                            row_count += 1
                            
                            # Debug: mostrar las primeras 3 filas
                            if row_count <= 3:
                                self.logger.info(f"ðŸ” DEBUGGING: Fila {row_count}: {dict(row)}")
                            
                            # Buscar columna que contenga URLs
                            url_found = False
                            for column_name in ['url', 'link', 'URL', 'Link']:
                                if column_name in row and row[column_name]:
                                    url = row[column_name].strip()
                                    if url.startswith('http'):
                                        urls.append(url)
                                        url_count += 1
                                        url_found = True
                                        
                                        # Debug: mostrar las primeras 5 URLs
                                        if url_count <= 5:
                                            self.logger.info(f"âœ… DEBUGGING: URL {url_count} encontrada: {url}")
                                        break
                            
                            if not url_found and row_count <= 3:
                                self.logger.warning(f"âš ï¸ DEBUGGING: No se encontrÃ³ URL vÃ¡lida en fila {row_count}")
                        
                        self.logger.info(f"ï¿½ DEBUGGING: Filas procesadas: {row_count}")
                        self.logger.info(f"ðŸŽ¯ DEBUGGING: URLs vÃ¡lidas extraÃ­das: {url_count}")
                        self.logger.info(f"ï¿½ðŸ“‚ Cargadas {len(urls)} URLs desde CSV {self.urls_file}")
                        
                        # Verificar si se extrajeron URLs
                        if len(urls) == 0:
                            self.logger.error(f"âŒ DEBUGGING: No se encontraron URLs vÃ¡lidas en el CSV")
                            self.logger.error(f"âŒ DEBUGGING: Verificar que existe columna 'url' con URLs vÃ¡lidas")
                        elif len(urls) < 50:
                            self.logger.warning(f"âš ï¸ DEBUGGING: Solo se encontraron {len(urls)} URLs, esperadas ~100")
                        
                        # Mostrar muestra de URLs extraÃ­das
                        if urls:
                            self.logger.info(f"ðŸ“‹ DEBUGGING: Muestra de URLs extraÃ­das:")
                            for i, url in enumerate(urls[:3]):
                                self.logger.info(f"   {i+1}. {url}")
                            if len(urls) > 3:
                                self.logger.info(f"   ... y {len(urls)-3} URLs mÃ¡s")
                
                else:
                    # Archivo de texto simple
                    self.logger.info(f"ðŸ“„ DEBUGGING: Detectado archivo de texto")
                    with open(self.urls_file, 'r', encoding='utf-8') as f:
                        all_lines = f.readlines()
                        self.logger.info(f"ðŸ“Š DEBUGGING: Total lÃ­neas en archivo: {len(all_lines)}")
                        
                        for line in all_lines:
                            url = line.strip()
                            if url and url.startswith('http'):
                                urls.append(url)
                        
                        self.logger.info(f"ðŸ“‚ Cargadas {len(urls)} URLs desde archivo de texto {self.urls_file}")
                        
            except Exception as e:
                self.logger.error(f"âŒ DEBUGGING ERROR cargando URLs: {e}")
                import traceback
                self.logger.error(f"âŒ DEBUGGING TRACEBACK: {traceback.format_exc()}")
        
        return urls
    
    def create_professional_driver(self):
        """
        Crear driver optimizado para Ubuntu Server sin interfaz grÃ¡fica
        """
        self.logger.info("ðŸ”§ Creando driver profesional optimizado...")
        
        # ConfiguraciÃ³n optimizada para Ubuntu Server headless
        sb_config = {
            'headless': True,  # Siempre headless para Ubuntu Server
            'uc': True,  # Usar undetected chrome
            'incognito': True,  # Modo incÃ³gnito
            'disable_csp': True,  # Deshabilitar Content Security Policy
            'disable_ws': True,  # Deshabilitar web security
            'block_images': False  # Permitir imÃ¡genes para mejor detecciÃ³n
        }
        
        return sb_config
    
    def load_checkpoint(self) -> Optional[Dict]:
        """Cargar checkpoint anterior si existe"""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'rb') as f:
                    checkpoint = pickle.load(f)
                self.logger.info(f"ðŸ“‚ Checkpoint cargado: propiedad {checkpoint.get('last_index', 0)}")
                return checkpoint
            except Exception as e:
                self.logger.warning(f"âš ï¸  Error cargando checkpoint: {e}")
        return None
    
    def save_checkpoint(self, index: int):
        """Guardar checkpoint del progreso actual"""
        checkpoint = {
            'last_index': index,
            'properties_processed': self.properties_processed,
            'successful_extractions': self.successful_extractions,
            'timestamp': datetime.now().isoformat(),
            'operation_type': self.operation_type
        }
        
        try:
            with open(self.checkpoint_file, 'wb') as f:
                pickle.dump(checkpoint, f)
            self.logger.info(f"ðŸ’¾ Checkpoint guardado: Ã­ndice {index}")
        except Exception as e:
            self.logger.error(f"âŒ Error guardando checkpoint: {e}")
    
    def extract_detailed_property_data(self, sb, url: str) -> Optional[Dict]:
        """
        Extraer datos detallados de una propiedad individual de inmuebles24
        Maneja tanto propiedades regulares como patrocinadas
        """
        try:
            # Verificar si es una propiedad patrocinada
            is_sponsored = self.is_sponsored_property(sb, url)
            
            # Datos bÃ¡sicos
            property_data = {
                'timestamp': datetime.now().isoformat(),
                'operation_type': self.operation_type,
                'property_url': url,
                'is_sponsored': is_sponsored
            }
            
            if is_sponsored:
                self.logger.info(f"ðŸ”¸ Detectada propiedad patrocinada: {url}")
                return self.extract_sponsored_property_data(sb, property_data)
            else:
                self.logger.info(f"ðŸ”¹ Procesando propiedad regular: {url}")
                return self.extract_regular_property_data(sb, property_data)
                
        except Exception as e:
            self.logger.error(f"âŒ Error extrayendo datos detallados: {e}")
            return None
    
    def is_sponsored_property(self, sb, url: str) -> bool:
        """
        Detectar si una propiedad es patrocinada basÃ¡ndose en indicadores comunes
        """
        try:
            # Indicadores de propiedades patrocinadas
            sponsored_indicators = [
                ".sponsored",
                ".patrocinado", 
                "[data-sponsored='true']",
                ".premium",
                ".destacado",
                ".featured"
            ]
            
            for selector in sponsored_indicators:
                if sb.is_element_present(selector):
                    return True
            
            # Verificar en el HTML si contiene palabras clave
            page_source = sb.get_page_source().lower()
            sponsored_keywords = ['patrocinado', 'sponsored', 'premium', 'destacado', 'featured']
            
            for keyword in sponsored_keywords:
                if keyword in page_source:
                    return True
                    
            # Verificar estructura diferente (propiedades patrocinadas suelen tener menos elementos)
            regular_elements = [
                "h1.title-property",
                ".property-features",
                ".property-description"
            ]
            
            missing_elements = 0
            for selector in regular_elements:
                if not sb.is_element_present(selector):
                    missing_elements += 1
            
            # Si faltan mÃ¡s de la mitad de elementos regulares, probablemente es patrocinada
            if missing_elements >= len(regular_elements) / 2:
                return True
                
            return False
            
        except Exception as e:
            self.logger.warning(f"âš ï¸  Error detectando propiedad patrocinada: {e}")
            return False
    
    def extract_sponsored_property_data(self, sb, url: str) -> Optional[Dict]:
        """
        Extraer datos de propiedades patrocinadas con selectores alternativos
        """
        try:
            # Inicializar diccionario de datos de propiedad
            property_data = {
                'url': url,
                'titulo': "N/A",
                'precio': "N/A",
                'ubicacion': "N/A",
                'ubicacion_url': "N/A",
                'tipo_propiedad': "N/A",
                'superficie_total': "N/A",
                'superficie_cubierta': "N/A",
                'habitaciones': "N/A",
                'banos': "N/A",
                'medio_banos_icon': "N/A",
                'cocheras': "N/A",
                'antiguedad_icon': "N/A",
                'caracteristicas': "N/A",
                'descripcion': "N/A",
                'contacto': "N/A",
                'telefono': "N/A",
                'operacion': "N/A",
                'mantenimiento': "N/A",
                'anunciante': "N/A",
                'codigo_anunciante': "N/A",
                'codigo_inmuebles24': "N/A",
                'tiempo_publicacion': "N/A"
            }
            
            # TÃ­tulo para propiedades patrocinadas
            try:
                sponsored_title_selectors = [
                    "h1",
                    "h2", 
                    ".title",
                    ".name",
                    "[class*='title']",
                    "[class*='name']"
                ]
                title = self.get_text_by_selectors(sb, sponsored_title_selectors)
                property_data['titulo'] = title
            except:
                property_data['titulo'] = "Propiedad Patrocinada - TÃ­tulo no disponible"
            
            # Precio para propiedades patrocinadas
            try:
                sponsored_price_selectors = [
                    "[class*='price']",
                    "[class*='precio']",
                    ".amount",
                    ".cost"
                ]
                price = self.get_text_by_selectors(sb, sponsored_price_selectors)
                property_data['precio'] = price
            except:
                property_data['precio'] = "Precio no disponible"
            
            # UbicaciÃ³n bÃ¡sica
            try:
                sponsored_location_selectors = [
                    "[class*='location']",
                    "[class*='address']",
                    "[class*='ubicacion']",
                    ".zone",
                    ".area"
                ]
                location = self.get_text_by_selectors(sb, sponsored_location_selectors)
                property_data['ubicacion'] = location
            except:
                property_data['ubicacion'] = "UbicaciÃ³n no disponible"
            
            # Campos especÃ­ficos para propiedades patrocinadas
            property_data['tipo_propiedad'] = "Patrocinada"
            property_data['ubicacion_url'] = "N/A"
            property_data['superficie_total'] = "N/A"
            property_data['superficie_cubierta'] = "N/A"
            property_data['habitaciones'] = "N/A"
            property_data['banos'] = "N/A"
            property_data['medio_banos_icon'] = "N/A"
            property_data['cocheras'] = "N/A"
            property_data['antiguedad_icon'] = "N/A"
            property_data['descripcion'] = "Propiedad patrocinada - DescripciÃ³n limitada"
            property_data['caracteristicas'] = "Ver detalles en la pÃ¡gina original"
            property_data['contacto'] = "N/A"
            property_data['telefono'] = "N/A"
            property_data['operacion'] = "N/A"
            property_data['mantenimiento'] = "N/A"
            property_data['anunciante'] = "N/A"
            property_data['codigo_anunciante'] = "N/A"
            property_data['codigo_inmuebles24'] = "N/A"
            property_data['tiempo_publicacion'] = "N/A"
            
            self.logger.info(f"âœ… Datos bÃ¡sicos extraÃ­dos de propiedad patrocinada")
            return property_data
            
        except Exception as e:
            self.logger.error(f"âŒ Error extrayendo propiedad patrocinada: {e}")
            return None
    
    def extract_regular_property_data(self, sb, url: str) -> Optional[Dict]:
        """
        Extraer datos de propiedades regulares usando selectores especÃ­ficos de inmuebles24
        """
        try:
            self.logger.info("ðŸ  Extrayendo datos de propiedad regular...")
            
            # Inicializar diccionario de datos de propiedad
            property_data = {
                'url': url,
                'titulo': "N/A",
                'precio': "N/A",
                'ubicacion': "N/A",
                'ubicacion_url': "N/A",  # Nueva variable del original
                'tipo_propiedad': "N/A",
                'superficie_total': "N/A",
                'superficie_cubierta': "N/A",
                'habitaciones': "N/A",
                'banos': "N/A",
                'medio_banos_icon': "N/A",  # Nueva variable del original
                'cocheras': "N/A",
                'antiguedad_icon': "N/A",  # Nueva variable del original
                'caracteristicas': "N/A",
                'descripcion': "N/A",
                'contacto': "N/A",
                'telefono': "N/A",  # Mejorar extracciÃ³n de telÃ©fono
                'operacion': "N/A",
                'mantenimiento': "N/A",
                'anunciante': "N/A",
                'codigo_anunciante': "N/A",
                'codigo_inmuebles24': "N/A",
                'tiempo_publicacion': "N/A"
            }
            
            # 1. TÃ­tulo principal
            try:
                title_element = sb.find_element("h1.title-property")
                property_data['titulo'] = title_element.text.strip()
            except:
                property_data['titulo'] = "N/A"
            
            # 2. Tipo de inmueble, Ã¡rea, recÃ¡maras y estacionamientos desde h2
            try:
                h2_element = sb.find_element("h2.title-type-sup-property")
                h2_text = h2_element.text
                tokens = [t.strip() for t in h2_text.replace("Â·", "|").split("|") if t.strip()]
                
                property_data['tipo_propiedad'] = tokens[0] if len(tokens) > 0 else "N/A"
                property_data['superficie_total'] = tokens[1] if len(tokens) > 1 else "N/A"
                
                # Extraer nÃºmero de recÃ¡maras
                if len(tokens) > 2:
                    import re
                    match = re.search(r"(\d+)", tokens[2])
                    property_data['habitaciones'] = match.group(1) if match else "N/A"
                
                # Extraer estacionamientos
                if len(tokens) > 3:
                    match = re.search(r"(\d+)", tokens[3])
                    property_data['cocheras'] = match.group(1) if match else "N/A"
                    
            except Exception as e:
                self.logger.warning(f"No se pudo extraer info del h2: {e}")
            
            # 3. Precio y operaciÃ³n
            try:
                price_container = sb.find_element(".price-container-property")
                price_value_div = price_container.find_element(By.CSS_SELECTOR, ".price-value")
                
                # Determinar operaciÃ³n
                price_text = price_value_div.text.lower()
                if "venta" in price_text:
                    property_data['operacion'] = "venta"
                elif "renta" in price_text:
                    property_data['operacion'] = "renta"
                
                # Extraer precio
                try:
                    span_precio = price_value_div.find_element(By.TAG_NAME, "span")
                    property_data['precio'] = span_precio.text.strip()
                except:
                    property_data['precio'] = "N/A"
                
                # Extraer mantenimiento/expensas
                try:
                    extra_div = price_container.find_element(By.CSS_SELECTOR, ".price-extra")
                    span_mant = extra_div.find_element(By.CSS_SELECTOR, ".price-expenses")
                    property_data['mantenimiento'] = span_mant.text.strip()
                except:
                    property_data['mantenimiento'] = "N/A"
                    
            except Exception as e:
                self.logger.warning(f"No se pudo extraer precio: {e}")
            
            # 4. DirecciÃ³n/UbicaciÃ³n y URL de ubicaciÃ³n
            try:
                location_div = sb.find_element(".section-location-property")
                h4_element = location_div.find_element(By.TAG_NAME, "h4")
                property_data['ubicacion'] = h4_element.text.strip()
                
                # Intentar extraer URL de ubicaciÃ³n/mapa
                try:
                    map_link = location_div.find_element(By.CSS_SELECTOR, "a[href*='maps'], a[href*='ubicacion'], a[href*='mapa']")
                    property_data['ubicacion_url'] = map_link.get_attribute('href')
                except:
                    property_data['ubicacion_url'] = "N/A"
                    
            except Exception as e:
                self.logger.warning(f"No se pudo extraer ubicaciÃ³n: {e}")
            
            # 5. DescripciÃ³n completa
            try:
                desc_section = sb.find_element("section.article-section-description")
                long_desc = desc_section.find_element(By.ID, "longDescription")
                property_data['descripcion'] = long_desc.text.strip()
            except Exception as e:
                self.logger.warning(f"No se pudo extraer descripciÃ³n: {e}")
            
            # 6. InformaciÃ³n del anunciante y contacto
            try:
                anunciante_element = sb.find_element('[data-qa="linkMicrositioAnunciante"]')
                property_data['anunciante'] = anunciante_element.text.strip()
                property_data['contacto'] = anunciante_element.text.strip()
                
                # Intentar extraer informaciÃ³n de telÃ©fono
                try:
                    # Buscar telÃ©fono en secciÃ³n de contacto
                    phone_selectors = [
                        '[data-qa="phone"]',
                        '.phone-number',
                        '.contact-phone',
                        '[href^="tel:"]',
                        '.telefono'
                    ]
                    
                    for selector in phone_selectors:
                        try:
                            phone_element = sb.find_element(selector)
                            phone_text = phone_element.text.strip() or phone_element.get_attribute('href')
                            if phone_text and ('tel:' in phone_text or any(c.isdigit() for c in phone_text)):
                                property_data['telefono'] = phone_text.replace('tel:', '')
                                break
                        except:
                            continue
                            
                except Exception as e:
                    self.logger.debug(f"No se pudo extraer telÃ©fono: {e}")
                    
            except Exception as e:
                self.logger.warning(f"No se pudo extraer anunciante: {e}")
            
            # 7. CÃ³digos del anuncio
            try:
                codes_section = sb.find_element("#reactPublisherCodes")
                lis = codes_section.find_elements(By.TAG_NAME, "li")
                
                for li in lis:
                    text = li.text.strip()
                    if "CÃ³d. del anunciante" in text:
                        parts = text.split(":")
                        property_data['codigo_anunciante'] = parts[1].strip() if len(parts) > 1 else "N/A"
                    elif "CÃ³d. Inmuebles24" in text:
                        parts = text.split(":")
                        property_data['codigo_inmuebles24'] = parts[1].strip() if len(parts) > 1 else "N/A"
            except Exception as e:
                self.logger.warning(f"No se pudieron extraer cÃ³digos: {e}")
            
            # 8. Tiempo de publicaciÃ³n
            try:
                user_views = sb.find_element("#user-views")
                p_element = user_views.find_element(By.TAG_NAME, "p")
                property_data['tiempo_publicacion'] = p_element.text.strip()
            except Exception as e:
                self.logger.warning(f"No se pudo extraer tiempo publicaciÃ³n: {e}")
            
            # 9. InformaciÃ³n detallada de iconos
            try:
                features_ul = sb.find_element("#section-icon-features-property")
                lis = features_ul.find_elements(By.CSS_SELECTOR, "li.icon-feature")
                
                for li in lis:
                    try:
                        icon = li.find_element(By.TAG_NAME, "i")
                        classes = icon.get_attribute("class") or ""
                        text = li.text.strip()
                        
                        if "icon-stotal" in classes:
                            property_data['superficie_total'] = text
                        elif "icon-scubierta" in classes:
                            property_data['superficie_cubierta'] = text
                        elif "icon-bano" in classes:
                            property_data['banos'] = text
                        elif "icon-cochera" in classes:
                            property_data['cocheras'] = text
                        elif "icon-dormitorio" in classes:
                            property_data['habitaciones'] = text
                        elif "icon-mediobano" in classes or "medio-bano" in classes:
                            property_data['medio_banos_icon'] = text
                        elif "icon-antiguedad" in classes or "antiguedad" in classes:
                            property_data['antiguedad_icon'] = text
                    except:
                        continue
                        
            except Exception as e:
                self.logger.warning(f"No se pudieron extraer caracterÃ­sticas detalladas: {e}")
            
            # 10. Extraer caracterÃ­sticas adicionales (botones expandibles)
            try:
                caracteristicas_list = []
                container = sb.find_element("#reactGeneralFeatures")
                buttons = container.find_elements(By.TAG_NAME, "button")
                
                for button in buttons:
                    try:
                        # Click en el botÃ³n para expandir
                        sb.execute_script("arguments[0].scrollIntoView(true);", button)
                        time.sleep(0.5)
                        sb.execute_script("arguments[0].click();", button)
                        time.sleep(1)
                        
                        # Extraer informaciÃ³n del contenido expandido
                        details_container = container.find_elements(By.TAG_NAME, "div")[1]
                        features = [elem.text.strip() for elem in details_container.find_elements(By.TAG_NAME, "span") if elem.text.strip()]
                        
                        button_text = button.find_element(By.TAG_NAME, "span").text.strip()
                        if features:
                            caracteristicas_list.append(f"{button_text}: {'; '.join(features)}")
                            
                    except:
                        continue
                
                property_data['caracteristicas'] = " | ".join(caracteristicas_list) if caracteristicas_list else "N/A"
                
            except Exception as e:
                self.logger.warning(f"No se pudieron extraer caracterÃ­sticas expandibles: {e}")
            
            # Verificar que se extrajo informaciÃ³n bÃ¡sica
            if property_data['titulo'] != "N/A" or property_data['precio'] != "N/A":
                self.logger.info(f"âœ… Datos regulares extraÃ­dos exitosamente")
                return property_data
            else:
                self.logger.warning(f"âš ï¸  No se pudo extraer informaciÃ³n bÃ¡sica de propiedad regular")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ Error extrayendo propiedad regular: {e}")
            return None
        """
        Extraer datos de propiedades regulares (no patrocinadas)
        """
        try:
            # TÃ­tulo principal
            try:
                title_selectors = [
                    "h1.title-property",
                    "h1.property-title",
                    "h1[data-qa='property-title']",
                    "h1"
                ]
                title = self.get_text_by_selectors(sb, title_selectors)
                property_data['titulo'] = title
            except:
                property_data['titulo'] = "N/A"
            
            # Precio
            try:
                price_selectors = [
                    ".price-property",
                    ".property-price",
                    "[data-qa='property-price']",
                    ".price",
                    ".precio"
                ]
                price = self.get_text_by_selectors(sb, price_selectors)
                property_data['precio'] = price
            except:
                property_data['precio'] = "N/A"
            
            # UbicaciÃ³n detallada
            try:
                location_selectors = [
                    ".location-property",
                    ".property-location",
                    "[data-qa='property-location']",
                    ".address",
                    ".ubicacion"
                ]
                location = self.get_text_by_selectors(sb, location_selectors)
                property_data['ubicacion'] = location
            except:
                property_data['ubicacion'] = "N/A"
            
            # Tipo de propiedad
            try:
                type_selectors = [
                    ".property-type",
                    "[data-qa='property-type']",
                    ".type"
                ]
                prop_type = self.get_text_by_selectors(sb, type_selectors)
                property_data['tipo_propiedad'] = prop_type
            except:
                property_data['tipo_propiedad'] = "N/A"
            
            # Superficie/Ãrea
            try:
                surface_selectors = [
                    ".surface-property",
                    ".property-surface",
                    "[data-qa='surface']",
                    ".superficie",
                    ".area"
                ]
                surface = self.get_text_by_selectors(sb, surface_selectors)
                property_data['superficie_total'] = surface
            except:
                property_data['superficie_total'] = "N/A"
            
            # CaracterÃ­sticas principales (habitaciones, baÃ±os, etc.)
            try:
                features_selectors = [
                    ".features-property",
                    ".property-features",
                    "[data-qa='features']",
                    ".characteristics"
                ]
                features_element = None
                for selector in features_selectors:
                    try:
                        features_element = sb.find_element(selector)
                        break
                    except:
                        continue
                
                if features_element:
                    # Extraer caracterÃ­sticas especÃ­ficas
                    feature_items = features_element.find_elements(By.CSS_SELECTOR, "li, .feature-item, .characteristic")
                    features = [item.text.strip() for item in feature_items if item.text.strip()]
                    property_data['caracteristicas'] = " | ".join(features)
                else:
                    # Buscar caracterÃ­sticas individuales
                    rooms = self.get_text_by_selectors(sb, [".rooms", ".bedrooms", ".habitaciones"])
                    bathrooms = self.get_text_by_selectors(sb, [".bathrooms", ".banos", ".baÃ±os"])
                    property_data['caracteristicas'] = f"Habitaciones: {rooms} | BaÃ±os: {bathrooms}"
            except:
                property_data['caracteristicas'] = "N/A"
            
            # DescripciÃ³n completa
            try:
                description_selectors = [
                    ".description-property",
                    ".property-description",
                    "[data-qa='description']",
                    ".description"
                ]
                description = self.get_text_by_selectors(sb, description_selectors)
                property_data['descripcion'] = description
            except:
                property_data['descripcion'] = "N/A"
            
            # InformaciÃ³n del contacto/agente
            try:
                contact_selectors = [
                    ".contact-property",
                    ".agent-info",
                    "[data-qa='contact']",
                    ".contact-info"
                ]
                contact = self.get_text_by_selectors(sb, contact_selectors)
                property_data['contacto'] = contact
            except:
                property_data['contacto'] = "N/A"
            
            # Campos adicionales para compatibilidad
            property_data['superficie_cubierta'] = "N/A"
            property_data['habitaciones'] = "N/A"
            property_data['banos'] = "N/A"
            property_data['cocheras'] = "N/A"
            property_data['telefono'] = "N/A"
            
            # Verificar que se extrajo al menos informaciÃ³n bÃ¡sica
            if property_data['titulo'] != "N/A" or property_data['precio'] != "N/A":
                self.logger.info(f"âœ… Datos regulares extraÃ­dos exitosamente")
                return property_data
            else:
                self.logger.warning(f"âš ï¸  No se pudo extraer informaciÃ³n bÃ¡sica de propiedad regular")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ Error extrayendo propiedad regular: {e}")
            return None

    def extract_detailed_property_data(self, sb, url):
        """
        MÃ©todo principal que determina el tipo de propiedad y extrae los datos
        """
        try:
            self.logger.info(f"ðŸ” Iniciando extracciÃ³n de datos para URL: {url}")
            
            # Verificar si es una propiedad patrocinada
            if self.is_sponsored_property(sb, url):
                self.logger.info("ðŸ“¢ Detectada propiedad patrocinada")
                return self.extract_sponsored_property_data(sb, url)
            else:
                self.logger.info("ðŸ  Detectada propiedad regular")
                return self.extract_regular_property_data(sb, url)
                
        except Exception as e:
            self.logger.error(f"âŒ Error extrayendo datos de {url}: {e}")
            return None

    def get_text_by_selectors(self, sb, selectors: List[str]) -> str:
        """Helper para probar mÃºltiples selectores y retornar el primer texto encontrado"""
        for selector in selectors:
            try:
                element = sb.find_element(selector)
                text = element.text.strip()
                if text:
                    return text
            except:
                continue
        return "N/A"
    
    def wait_and_check_blocking(self, sb, timeout=15) -> bool:
        """
        Verificar si la pÃ¡gina estÃ¡ bloqueada o cargada correctamente
        EspecÃ­fico para inmuebles24
        """
        try:
            # Esperar a que carguen elementos de la pÃ¡gina de detalle
            WebDriverWait(sb.driver, timeout).until(
                lambda driver: (
                    driver.find_elements(By.CSS_SELECTOR, "h1") or
                    driver.find_elements(By.CSS_SELECTOR, ".title-property") or
                    driver.find_elements(By.CSS_SELECTOR, "#challenge-form") or
                    driver.find_elements(By.CSS_SELECTOR, ".cf-browser-verification")
                )
            )
            
            # Verificar elementos de bloqueo especÃ­ficos de inmuebles24
            blocking_selectors = [
                "#challenge-form",
                ".cf-browser-verification", 
                ".cf-checking-browser",
                "title:contains('Just a moment')",
                "h1:contains('Checking your browser')",
                ".captcha",
                "#captcha",
                ".access-denied",
                ".blocked-access"
            ]
            
            for selector in blocking_selectors:
                if sb.is_element_visible(selector):
                    self.logger.warning(f"ðŸš« PÃ¡gina bloqueada - detectado: {selector}")
                    return False
            
            # Verificar si la pÃ¡gina tiene contenido de propiedad
            content_found = (
                sb.find_elements("h1") or
                sb.find_elements(".title-property") or
                sb.find_elements(".property-title") or
                sb.find_elements(".price-property")
            )
            
            if content_found:
                self.logger.debug("âœ… PÃ¡gina de detalle cargada correctamente")
                return True
            else:
                self.logger.warning("âš ï¸  PÃ¡gina cargada pero sin contenido detectado")
                return False
                
        except TimeoutException:
            self.logger.error("âŒ Timeout esperando que cargue la pÃ¡gina de detalle")
            return False
        except Exception as e:
            self.logger.error(f"âŒ Error verificando bloqueo: {e}")
            return False
    
    def scrape_properties(self) -> Tuple[int, int]:
        """
        MÃ©todo principal de scraping de propiedades individuales
        Retorna (total_processed, successful_extractions)
        """
        self.start_time = datetime.now()
        
        if not self.property_urls:
            self.logger.error("âŒ No hay URLs para procesar")
            return 0, 0
        
        # DEBUGGING: Verificar URLs cargadas
        self.logger.info(f"ðŸ” DEBUGGING SCRAPING: URLs cargadas para procesar:")
        self.logger.info(f"ðŸ“Š DEBUGGING: Total URLs disponibles: {len(self.property_urls)}")
        self.logger.info(f"ðŸŽ¯ DEBUGGING: Max properties configurado: {self.max_properties}")
        self.logger.info(f"ðŸ”„ DEBUGGING: Resume from: {self.resume_from}")
        
        # Mostrar las primeras 5 URLs para verificar
        for i, url in enumerate(self.property_urls[:5]):
            self.logger.info(f"ðŸ“‹ DEBUGGING: URL {i+1}: {url}")
        if len(self.property_urls) > 5:
            self.logger.info(f"ðŸ“‹ DEBUGGING: ... y {len(self.property_urls)-5} URLs mÃ¡s")
        
        # Cargar checkpoint si existe
        checkpoint = self.load_checkpoint()
        if checkpoint and self.resume_from == 0:
            self.resume_from = checkpoint.get('last_index', 0)
            self.logger.info(f"ðŸ”„ Resumiendo desde Ã­ndice {self.resume_from}")
        
        with SB(**self.create_professional_driver()) as sb:
            
            start_index = self.resume_from
            end_index = len(self.property_urls)
            
            if self.max_properties:
                end_index = min(start_index + self.max_properties, end_index)
            
            # DEBUGGING: Mostrar rango de procesamiento
            self.logger.info(f"ðŸŽ¯ DEBUGGING RANGE: Procesando desde Ã­ndice {start_index} hasta {end_index}")
            self.logger.info(f"ðŸŽ¯ DEBUGGING RANGE: URLs que se procesarÃ¡n:")
            for idx in range(start_index, min(start_index + 5, end_index)):
                if idx < len(self.property_urls):
                    self.logger.info(f"   {idx+1}. {self.property_urls[idx]}")
            
            consecutive_failures = 0
            max_consecutive_failures = 10
            
            for i in range(start_index, end_index):
                # DEBUGGING: Verificar que el Ã­ndice estÃ© en rango
                if i >= len(self.property_urls):
                    self.logger.error(f"âŒ DEBUGGING: Ãndice {i} fuera de rango. Total URLs: {len(self.property_urls)}")
                    break
                
                url = self.property_urls[i]
                
                # DEBUGGING: InformaciÃ³n detallada de cada iteraciÃ³n
                self.logger.info(f"ðŸ” DEBUGGING ITERACIÃ“N {i+1}:")
                self.logger.info(f"   ðŸ“Œ Ãndice actual: {i}")
                self.logger.info(f"   ðŸŒ URL actual: {url}")
                self.logger.info(f"   ðŸ“Š Progreso: {i+1}/{end_index} (de {len(self.property_urls)} totales)")
                
                try:
                    self.logger.info(f"ðŸ  Procesando propiedad {i+1}/{len(self.property_urls)}: {url}")
                    
                    # DEBUGGING: Verificar navegaciÃ³n
                    self.logger.info(f"ðŸŒ DEBUGGING: Navegando a URL: {url}")
                    
                    # Navegar a la pÃ¡gina de la propiedad
                    sb.open(url)
                    
                    # Pausa para carga (crucial para inmuebles24)
                    self.logger.info(f"â±ï¸ DEBUGGING: Esperando 4 segundos para carga de pÃ¡gina...")
                    time.sleep(4)
                    
                    # DEBUGGING: Verificar pÃ¡gina cargada
                    current_url = sb.get_current_url()
                    self.logger.info(f"âœ… DEBUGGING: PÃ¡gina cargada, URL actual: {current_url}")
                    
                    # Verificar bloqueo y esperar carga
                    self.logger.info(f"ðŸ”’ DEBUGGING: Verificando bloqueo y carga de pÃ¡gina...")
                    if not self.wait_and_check_blocking(sb):
                        self.logger.warning(f"âš ï¸ DEBUGGING: FallÃ³ verificaciÃ³n de bloqueo para URL {url}")
                        consecutive_failures += 1
                        self.errors_count += 1
                        
                        if consecutive_failures >= max_consecutive_failures:
                            self.logger.error(f"âŒ Demasiados fallos consecutivos ({consecutive_failures}). Deteniendo.")
                            break
                        
                        self.logger.warning(f"âš ï¸  Propiedad {i+1} fallÃ³. Continuando...")
                        time.sleep(8)  # Pausa mÃ¡s larga para inmuebles24
                        continue
                    
                    # Extraer datos de la propiedad
                    property_data = self.extract_detailed_property_data(sb, url)
                    
                    if property_data:
                        self.properties_data.append(property_data)
                        self.successful_extractions += 1
                        consecutive_failures = 0  # Reset contador
                        self.logger.info(f"âœ… Datos extraÃ­dos exitosamente para propiedad {i+1}")
                    else:
                        consecutive_failures += 1
                        self.errors_count += 1
                        self.logger.warning(f"âš ï¸  No se pudieron extraer datos de propiedad {i+1}")
                    
                    self.properties_processed += 1
                    
                    # Guardar checkpoint cada N propiedades
                    if (i + 1) % self.checkpoint_interval == 0:
                        self.save_checkpoint(i)
                    
                    # Log de progreso
                    elapsed = datetime.now() - self.start_time
                    avg_time_per_property = elapsed.total_seconds() / self.properties_processed
                    success_rate = (self.successful_extractions / self.properties_processed) * 100
                    
                    self.logger.info(f"ðŸ“Š Progreso - Procesadas: {self.properties_processed} | Exitosas: {self.successful_extractions} | Tasa Ã©xito: {success_rate:.1f}% | Tiempo: {avg_time_per_property:.1f}s/prop")
                    
                    # Pausa entre propiedades (anti-detecciÃ³n para inmuebles24)
                    time.sleep(3)
                    
                except KeyboardInterrupt:
                    self.logger.info("â¹ï¸  Scraping interrumpido por usuario")
                    self.save_checkpoint(i)
                    break
                    
                except Exception as e:
                    consecutive_failures += 1
                    self.errors_count += 1
                    self.logger.error(f"âŒ Error procesando propiedad {i+1}: {e}")
                    
                    if consecutive_failures >= max_consecutive_failures:
                        self.logger.error("âŒ Demasiados errores consecutivos. Deteniendo.")
                        break
                    
                    time.sleep(8)
        
        return self.properties_processed, self.successful_extractions
    
    def get_script_number(self, month_abbrev, year_short):
        """Detectar automÃ¡ticamente si es la primera (1) o segunda (2) ejecuciÃ³n del mes"""
        month_year_folder = f"{month_abbrev}{year_short}"
        execution_dir_1 = self.data_dir / month_year_folder / "1"
        
        # Si existe la carpeta 1 y tiene archivos CSV, entonces esta es la segunda ejecuciÃ³n
        if execution_dir_1.exists():
            csv_files = list(execution_dir_1.glob("I24_URLs_*.csv"))
            if csv_files:
                return "2"  # Segunda ejecuciÃ³n del mes
        
        return "1"  # Primera ejecuciÃ³n del mes

    def save_results(self) -> str:
        """Guardar resultados en formato CSV con metadata"""
        if not self.properties_data:
            self.logger.warning("âš ï¸  No hay datos para guardar")
            return None
        
        # Generar timestamp para archivos
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Determinar carpeta de destino con nueva nomenclatura
        current_date = datetime.now()
        month_abbrev = {
            1: 'ene', 2: 'feb', 3: 'mar', 4: 'abr', 5: 'may', 6: 'jun',
            7: 'jul', 8: 'ago', 9: 'sep', 10: 'oct', 11: 'nov', 12: 'dic'
        }[current_date.month]
        year_short = str(current_date.year)[-2:]  # Ãšltimos 2 dÃ­gitos del aÃ±o
        
        # Determinar si es la primera (1) o segunda (2) ejecuciÃ³n del mes automÃ¡ticamente
        script_number = self.get_script_number(month_abbrev, year_short)
        
        # Crear estructura de carpetas: inm24/ven/ene26/1/
        month_year_folder = f"{month_abbrev}{year_short}"  # ene26, dic25, etc.
        script_folder = script_number  # 1 o 2
        execution_dir = self.data_dir / month_year_folder / script_folder
        execution_dir.mkdir(parents=True, exist_ok=True)
        
        # Archivo CSV de detalles con nuevo formato: inm24_ene26_1.csv
        csv_filename = f"inm24_{month_abbrev}{year_short}_{script_number}.csv"
        csv_path = execution_dir / csv_filename
        
        # Guardar CSV
        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            if self.properties_data:
                fieldnames = self.properties_data[0].keys()
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(self.properties_data)
        
        # Metadata
        metadata = {
            'execution_info': {
                'timestamp': timestamp,
                'operation_type': self.operation_type,
                'total_urls_provided': len(self.property_urls),
                'properties_processed': self.properties_processed,
                'successful_extractions': self.successful_extractions,
                'errors_count': self.errors_count,
                'execution_time_seconds': (datetime.now() - self.start_time).total_seconds(),
                'csv_filename': csv_filename,
                'log_filename': self.log_file.name,
                'urls_file_used': str(self.urls_file) if self.urls_file else None
            },
            'system_info': {
                'scraper_version': '1.0.0',
                'scraper_type': 'detailed_individual_properties',
                'python_version': sys.version,
                'headless_mode': self.headless,
                'max_properties_limit': self.max_properties,
                'resume_from_index': self.resume_from
            }
        }
        
        metadata_path = execution_dir / f"metadata_unico_{timestamp}.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ðŸ’¾ Resultados guardados:")
        self.logger.info(f"   ðŸ“„ CSV Detalles: {csv_path}")
        self.logger.info(f"   ðŸ“‹ Metadata: {metadata_path}")
        
        # Limpiar checkpoint al finalizar exitosamente
        if self.checkpoint_file.exists():
            self.checkpoint_file.unlink()
            self.logger.info("ðŸ—‘ï¸  Checkpoint limpiado")
        
        return str(csv_path)
    
    def run(self) -> Dict:
        """Ejecutar scraping completo y retornar resultados"""
        self.logger.info("ðŸš€ Iniciando scraping detallado de propiedades Inmuebles24")
        self.logger.info("="*70)
        
        try:
            # Ejecutar scraping
            properties_processed, successful_extractions = self.scrape_properties()
            
            if properties_processed == 0:
                return {
                    'success': False,
                    'error': 'No se procesaron propiedades',
                    'properties_processed': 0,
                    'successful_extractions': 0
                }
            
            # Guardar resultados
            csv_path = self.save_results()
            
            # Calcular estadÃ­sticas finales
            total_time = datetime.now() - self.start_time
            avg_time_per_property = total_time.total_seconds() / max(properties_processed, 1)
            success_rate = (successful_extractions / max(properties_processed, 1)) * 100
            
            results = {
                'success': True,
                'properties_processed': properties_processed,
                'successful_extractions': successful_extractions,
                'errors_count': self.errors_count,
                'total_time_seconds': total_time.total_seconds(),
                'avg_time_per_property': avg_time_per_property,
                'success_rate': success_rate,
                'csv_file': csv_path,
                'operation_type': self.operation_type
            }
            
            # Log final
            self.logger.info("="*70)
            self.logger.info("ðŸŽ‰ SCRAPING DETALLADO COMPLETADO EXITOSAMENTE")
            self.logger.info(f"ðŸ“Š Propiedades procesadas: {properties_processed}")
            self.logger.info(f"âœ… Extracciones exitosas: {successful_extractions}")
            self.logger.info(f"âŒ Errores: {self.errors_count}")
            self.logger.info(f"â±ï¸  Tiempo total: {total_time}")
            self.logger.info(f"âš¡ Promedio por propiedad: {avg_time_per_property:.1f}s")
            self.logger.info(f"âœ… Tasa de Ã©xito: {success_rate:.1f}%")
            self.logger.info("="*70)
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ Error fatal en scraping: {e}")
            return {
                'success': False,
                'error': str(e),
                'properties_processed': self.properties_processed,
                'successful_extractions': self.successful_extractions
            }

def main():
    """FunciÃ³n principal con argumentos de lÃ­nea de comandos"""
    parser = argparse.ArgumentParser(description='Inmuebles24 Unico Professional Scraper')
    parser.add_argument('--urls-file', type=str, default=None,
                       help='Archivo con URLs de propiedades a procesar')
    parser.add_argument('--headless', action='store_true', default=True, 
                       help='Ejecutar en modo headless (sin GUI)')
    parser.add_argument('--properties', type=int, default=None, 
                       help='NÃºmero mÃ¡ximo de propiedades a procesar')
    parser.add_argument('--resume', type=int, default=0, 
                       help='Ãndice desde el cual resumir')
    parser.add_argument('--operation', choices=['venta', 'renta'], default='venta',
                       help='Tipo de operaciÃ³n: venta o renta')
    parser.add_argument('--gui', action='store_true', 
                       help='Ejecutar con GUI (opuesto a --headless)')
    
    args = parser.parse_args()
    
    # Ajustar headless basado en argumentos
    # Por defecto es headless para Ubuntu Server, GUI solo si se especifica --gui
    if args.gui:
        args.headless = False
    else:
        args.headless = True
    
    # Crear y ejecutar scraper
    scraper = Inmuebles24UnicoProfessionalScraper(
        urls_file=args.urls_file,
        headless=args.headless,
        max_properties=args.properties,
        resume_from=args.resume,
        operation_type=args.operation
    )
    
    results = scraper.run()
    
    # Retornar cÃ³digo de salida apropiado
    sys.exit(0 if results['success'] else 1)

if __name__ == "__main__":
    main()
